{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.chrome.service import Service\\nfrom webdriver_manager.chrome import ChromeDriverManager\\nfrom selenium.webdriver.common.action_chains import ActionChains\\nfrom selenium.webdriver.chrome.options import Options\\nfrom selenium.webdriver.support.ui import WebDriverWait\\nfrom selenium.webdriver.support import expected_conditions as EC\\nimport time\\nfrom bs4 import BeautifulSoup\\n\\noptions = Options()\\n\\n# Setup Chrome WebDriver\\nservice = Service(ChromeDriverManager().install())\\ndriver = webdriver.Chrome(service=service, options=options)\\n\\n# List of search query URLs\\nquery_urls = [\\n    \"https://www.kompas.id/search?q=kecelakaan&from=01%20Jan%202017&to=12%20Nov%202024&dateRange=customize\",\\n    \"https://www.kompas.id/search?q=tabrakan\",\\n    \"https://www.kompas.id/search?q=laka%20lantas\"\\n]\\n\\n# Function to zoom out the page\\ndef zoom_out(driver, zoom_factor=0.8):\\n    driver.execute_script(f\"document.body.style.zoom=\\'{zoom_factor}\\'\")\\n    print(f\"Zoomed out to {zoom_factor * 100}%\")\\n\\n# Zoom out twice to ensure the button is visible\\nzoom_out(driver, 0.8)  # First zoom out (80% zoom)\\nzoom_out(driver, 0.6)  # Second zoom out (60% zoom)\\n\\n# Function to scroll to the bottom of the page\\ndef scroll_to_bottom(driver):\\n    last_height = driver.execute_script(\"return window.screen.height;\")\\n    i = 1\\n    scroll_pause_time = 1\\n    while True:\\n        driver.execute_script(f\"window.scrollTo(0, {last_height * i});\")\\n        i += 1\\n        time.sleep(scroll_pause_time)\\n        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\\n        if last_height * i > scroll_height:\\n            break\\n\\n# Function to load more content manually by clicking the \"Lihat Lainnya\" button\\ndef click_load_more_button(driver):\\n    try:\\n        # Wait for the button to become clickable\\n        load_more_button = WebDriverWait(driver, 10).until(\\n            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, \\'kui-NhM\\') and contains(text(), \\'Lihat Lainnya\\')]\"))\\n        )\\n        \\n        # Scroll to the \"Lihat Lainnya\" button to ensure it\\'s in view\\n        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\\n        time.sleep(1)  # Small delay to ensure button is in view\\n\\n        # Log for debugging\\n        print(\"Clicking \\'Lihat Lainnya\\' button...\")\\n\\n        # Click the button to load more content\\n        ActionChains(driver).move_to_element(load_more_button).click(load_more_button).perform()\\n        time.sleep(3)  # Wait for the content to load\\n\\n        # Log if more content is loading\\n        print(\"More content should have been loaded now.\")\\n\\n    except Exception as e:\\n        print(\"Error or no more content:\", e)\\n\\n# Function to extract article URLs from the page\\ndef extract_article_urls(driver):\\n    soup = BeautifulSoup(driver.page_source, \\'html.parser\\')\\n    article_links = []\\n    for link in soup.find_all(\\'a\\', href=True):\\n        url = link[\\'href\\']\\n        if \"/baca/\" in url:  # Only collect article links\\n            article_links.append(url)\\n    \\n    return list(set(article_links))  # Remove duplicates\\n\\n# Store already collected URLs to prevent duplicates\\ncollected_urls = set()\\n\\n# Function to handle collection for a single query URL\\ndef collect_urls_for_query(query_url):\\n    # Go to the page\\n    driver.get(query_url)\\n    \\n    # Extract initial article URLs\\n    initial_urls = extract_article_urls(driver)\\n    collected_urls.update(initial_urls)\\n\\n    # Print initial collected URLs\\n    print(f\"Initial Collected URLs for {query_url}:\")\\n    for url in initial_urls:\\n        print(url)\\n\\n    # Save the URLs in a file\\n    with open(\"article_urls.txt\", \\'a\\') as file:\\n        for url in initial_urls:\\n            file.write(url + \"\\n\")\\n\\n    # Infinite loop to monitor the page, collect URLs, and check for manual clicks\\n    try:\\n        while True:\\n            # Extract and save any new URLs\\n            new_urls = extract_article_urls(driver)\\n            new_urls = [url for url in new_urls if url not in collected_urls]\\n            \\n            if new_urls:\\n                collected_urls.update(new_urls)\\n                print(\"New URLs Collected:\")\\n                for url in new_urls:\\n                    print(url)\\n                \\n                # Append new URLs to the file\\n                with open(\"article_urls.txt\", \\'a\\') as file:\\n                    for url in new_urls:\\n                        file.write(url + \"\\n\")\\n\\n            # Wait for a few seconds before checking for new content again\\n            print(\"Waiting for new content...\")\\n            time.sleep(5)  # Adjust this to your needs\\n\\n            # Allow user to manually click to load more content\\n            click_load_more_button(driver)\\n\\n            # Break if there is no more content to load (this ensures we move to the next query)\\n            # You can add a more specific condition here, like detecting if a \"No more results\" message appears\\n            if not new_urls:\\n                print(\"No new URLs found. Moving to the next query.\")\\n                break\\n\\n    except KeyboardInterrupt:\\n        print(\"Manual stop received for this query. Moving to the next query.\")\\n\\n# Loop over all the query URLs and collect data for each\\nfor query_url in query_urls:\\n    collect_urls_for_query(query_url)\\n    \\n    # Wait for 30 seconds to make sure we\\'ve reached the bottom of the page before switching\\n    print(\"Waiting for 30 seconds before moving to the next query...\")\\n    time.sleep(30)\\n\\n# Close the driver when done\\ndriver.quit()\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = Options()\n",
    "\n",
    "# Setup Chrome WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# List of search query URLs\n",
    "query_urls = [\n",
    "    \"https://www.kompas.id/search?q=kecelakaan&from=01%20Jan%202017&to=12%20Nov%202024&dateRange=customize\",\n",
    "    \"https://www.kompas.id/search?q=tabrakan\",\n",
    "    \"https://www.kompas.id/search?q=laka%20lantas\"\n",
    "]\n",
    "\n",
    "# Function to zoom out the page\n",
    "def zoom_out(driver, zoom_factor=0.8):\n",
    "    driver.execute_script(f\"document.body.style.zoom='{zoom_factor}'\")\n",
    "    print(f\"Zoomed out to {zoom_factor * 100}%\")\n",
    "\n",
    "# Zoom out twice to ensure the button is visible\n",
    "zoom_out(driver, 0.8)  # First zoom out (80% zoom)\n",
    "zoom_out(driver, 0.6)  # Second zoom out (60% zoom)\n",
    "\n",
    "# Function to scroll to the bottom of the page\n",
    "def scroll_to_bottom(driver):\n",
    "    last_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "    scroll_pause_time = 1\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {last_height * i});\")\n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if last_height * i > scroll_height:\n",
    "            break\n",
    "\n",
    "# Function to load more content manually by clicking the \"Lihat Lainnya\" button\n",
    "def click_load_more_button(driver):\n",
    "    try:\n",
    "        # Wait for the button to become clickable\n",
    "        load_more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'kui-NhM') and contains(text(), 'Lihat Lainnya')]\"))\n",
    "        )\n",
    "        \n",
    "        # Scroll to the \"Lihat Lainnya\" button to ensure it's in view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        time.sleep(1)  # Small delay to ensure button is in view\n",
    "\n",
    "        # Log for debugging\n",
    "        print(\"Clicking 'Lihat Lainnya' button...\")\n",
    "\n",
    "        # Click the button to load more content\n",
    "        ActionChains(driver).move_to_element(load_more_button).click(load_more_button).perform()\n",
    "        time.sleep(3)  # Wait for the content to load\n",
    "\n",
    "        # Log if more content is loading\n",
    "        print(\"More content should have been loaded now.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error or no more content:\", e)\n",
    "\n",
    "# Function to extract article URLs from the page\n",
    "def extract_article_urls(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    article_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        url = link['href']\n",
    "        if \"/baca/\" in url:  # Only collect article links\n",
    "            article_links.append(url)\n",
    "    \n",
    "    return list(set(article_links))  # Remove duplicates\n",
    "\n",
    "# Store already collected URLs to prevent duplicates\n",
    "collected_urls = set()\n",
    "\n",
    "# Function to handle collection for a single query URL\n",
    "def collect_urls_for_query(query_url):\n",
    "    # Go to the page\n",
    "    driver.get(query_url)\n",
    "    \n",
    "    # Extract initial article URLs\n",
    "    initial_urls = extract_article_urls(driver)\n",
    "    collected_urls.update(initial_urls)\n",
    "\n",
    "    # Print initial collected URLs\n",
    "    print(f\"Initial Collected URLs for {query_url}:\")\n",
    "    for url in initial_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save the URLs in a file\n",
    "    with open(\"article_urls.txt\", 'a') as file:\n",
    "        for url in initial_urls:\n",
    "            file.write(url + \"\\n\")\n",
    "\n",
    "    # Infinite loop to monitor the page, collect URLs, and check for manual clicks\n",
    "    try:\n",
    "        while True:\n",
    "            # Extract and save any new URLs\n",
    "            new_urls = extract_article_urls(driver)\n",
    "            new_urls = [url for url in new_urls if url not in collected_urls]\n",
    "            \n",
    "            if new_urls:\n",
    "                collected_urls.update(new_urls)\n",
    "                print(\"New URLs Collected:\")\n",
    "                for url in new_urls:\n",
    "                    print(url)\n",
    "                \n",
    "                # Append new URLs to the file\n",
    "                with open(\"article_urls.txt\", 'a') as file:\n",
    "                    for url in new_urls:\n",
    "                        file.write(url + \"\\n\")\n",
    "\n",
    "            # Wait for a few seconds before checking for new content again\n",
    "            print(\"Waiting for new content...\")\n",
    "            time.sleep(5)  # Adjust this to your needs\n",
    "\n",
    "            # Allow user to manually click to load more content\n",
    "            click_load_more_button(driver)\n",
    "\n",
    "            # Break if there is no more content to load (this ensures we move to the next query)\n",
    "            # You can add a more specific condition here, like detecting if a \"No more results\" message appears\n",
    "            if not new_urls:\n",
    "                print(\"No new URLs found. Moving to the next query.\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Manual stop received for this query. Moving to the next query.\")\n",
    "\n",
    "# Loop over all the query URLs and collect data for each\n",
    "for query_url in query_urls:\n",
    "    collect_urls_for_query(query_url)\n",
    "    \n",
    "    # Wait for 30 seconds to make sure we've reached the bottom of the page before switching\n",
    "    print(\"Waiting for 30 seconds before moving to the next query...\")\n",
    "    time.sleep(30)\n",
    "\n",
    "# Close the driver when done\n",
    "driver.quit()\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query : `kecelakaan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "df_kecelakaan = pd.DataFrame()\n",
    "\n",
    "urls = set()\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "for i in range(100):\n",
    "    url = f\"https://www.detik.com/search/searchall?query=kecelakaan&page={i+1}&result_type=relevansi&siteid=3&fromdatex=01/01/2014&todatex=13/11/2024\"\n",
    "    driver.get(url)\n",
    "\n",
    "    scroll_pause_time = 2\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    j=1\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height*j});\")\n",
    "        j += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if screen_height * j > scroll_height:\n",
    "            break\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for link in soup.find_all('a', class_='media__link', href=True):\n",
    "        article = link['href']\n",
    "        if \"/berita/\"  in article:\n",
    "            urls.add(article)\n",
    "       \n",
    "df_kecelakaan['url'] = list(urls)\n",
    "\n",
    "df_kecelakaan.to_csv('url_detik.csv', index=False)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://news.detik.com/berita/d-7479347/truk-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://news.detik.com/berita/d-7536845/sopir-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://news.detik.com/berita/d-7118253/kecela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://news.detik.com/berita/d-7538747/tak-ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://news.detik.com/berita/d-7419249/polisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>https://news.detik.com/berita/d-7293220/tangis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>https://news.detik.com/berita/d-7358961/lalin-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>https://news.detik.com/berita/d-7335558/datang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>https://news.detik.com/berita/d-7565906/halte-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>https://news.detik.com/berita/d-7633246/kecela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url\n",
       "0    https://news.detik.com/berita/d-7479347/truk-l...\n",
       "1    https://news.detik.com/berita/d-7536845/sopir-...\n",
       "2    https://news.detik.com/berita/d-7118253/kecela...\n",
       "3    https://news.detik.com/berita/d-7538747/tak-ku...\n",
       "4    https://news.detik.com/berita/d-7419249/polisi...\n",
       "..                                                 ...\n",
       "747  https://news.detik.com/berita/d-7293220/tangis...\n",
       "748  https://news.detik.com/berita/d-7358961/lalin-...\n",
       "749  https://news.detik.com/berita/d-7335558/datang...\n",
       "750  https://news.detik.com/berita/d-7565906/halte-...\n",
       "751  https://news.detik.com/berita/d-7633246/kecela...\n",
       "\n",
       "[752 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query = `tabrakan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabrakan = pd.DataFrame()\n",
    "\n",
    "urls = set()\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "for i in range(100):\n",
    "    url = f\"https://www.detik.com/search/searchall?query=tabrakan&page={1+i}&result_type=relevansi&siteid=3&fromdatex=01/01/2014&todatex=13/11/2024\"\n",
    "    driver.get(url)\n",
    "\n",
    "    scroll_pause_time = 2\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    j=1\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height*j});\")\n",
    "        j += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if screen_height * j > scroll_height:\n",
    "            break\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for link in soup.find_all('a', class_='media__link', href=True):\n",
    "        article = link['href']\n",
    "        if \"/berita/\"  in article:\n",
    "            urls.add(article)\n",
    "       \n",
    "df_tabrakan['url'] = list(urls)\n",
    "\n",
    "df_tabrakan.to_csv('url_detik_tabrakan.csv', index=False)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://news.detik.com/berita/d-6709913/anak-k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://news.detik.com/berita/d-7370729/pengen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://news.detik.com/berita/d-7125507/8-perj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://news.detik.com/berita/d-6770135/ada-4-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://news.detik.com/berita/d-7156516/dindin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>https://news.detik.com/berita/d-7298344/tabrak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>https://news.detik.com/berita/d-6702772/tabrak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>https://news.detik.com/berita/d-7565906/halte-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>https://news.detik.com/berita/d-7183767/dampak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>https://news.detik.com/berita/d-6839555/kecela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url\n",
       "0    https://news.detik.com/berita/d-6709913/anak-k...\n",
       "1    https://news.detik.com/berita/d-7370729/pengen...\n",
       "2    https://news.detik.com/berita/d-7125507/8-perj...\n",
       "3    https://news.detik.com/berita/d-6770135/ada-4-...\n",
       "4    https://news.detik.com/berita/d-7156516/dindin...\n",
       "..                                                 ...\n",
       "678  https://news.detik.com/berita/d-7298344/tabrak...\n",
       "679  https://news.detik.com/berita/d-6702772/tabrak...\n",
       "680  https://news.detik.com/berita/d-7565906/halte-...\n",
       "681  https://news.detik.com/berita/d-7183767/dampak...\n",
       "682  https://news.detik.com/berita/d-6839555/kecela...\n",
       "\n",
       "[683 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tabrakan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
